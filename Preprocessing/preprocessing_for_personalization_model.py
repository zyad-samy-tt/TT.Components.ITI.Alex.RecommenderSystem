# -*- coding: utf-8 -*-
"""preprocessing for 3rd model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PQNZ_8XvDgetHM3hoYh2d45JZNHNKbRo

# Data Processing Pipeline For 3rd Model Of Personalization

## Introduction
This document provides a detailed description of the steps involved in the data processing pipeline for cleaning, extracting, and merging user data. The pipeline is implemented in Python and includes functions for downloading files, loading data, cleaning data, extracting age information, filling missing values, and saving the final processed data.

---

## 1. Downloading Files

**Function**: `download_files`

- **Purpose**: Downloads the required files for the analysis from Google Drive.
- **Details**:
  - Downloads files with specified IDs and saves them with given filenames.

## 2. Loading Data

**Function**: `load_data`

- **Purpose**: Loads the main data and the CSV files into dataframes.
- **Details**:
  - Reads the main data file and the two CSV files (`after1.csv` and `before.csv`) into separate dataframes.

## 3. Cleaning Data

**Function**: `clean_data`

- **Purpose**: Cleans the dataframe by dropping duplicates and rows with missing 'Answer' values.
- **Details**:
  - Removes duplicate rows.
  - Drops rows where the 'Answer' column has missing values.

## 4. Extracting Age

**Function**: `extract_age`

- **Purpose**: Extracts age from the 'Answer' column, which contains the birthdate.
- **Details**:
  - Extracts birthdate from the 'Answer' column using a regular expression.
  - Calculates age based on the extracted birthdate.

## 5. Adding Age and Age Group

**Function**: `add_age_and_age_group`

- **Purpose**: Adds 'age' and 'age_group' columns to the cleaned dataframe.
- **Details**:
  - Applies the `extract_age` function to the 'Answer' column.
  - Categorizes ages into predefined age groups.

## 6. Extracting Unique Users

**Function**: `extract_unique_users`

- **Purpose**: Extracts unique users with relevant information.
- **Details**:
  - Selects relevant columns.
  - Drops rows with missing 'User_ID' values.
  - Removes duplicate rows based on 'User_ID', 'age', 'city', and 'region'.

## 7. Filling Missing Ages

**Function**: `fill_missing_ages`

- **Purpose**: Fills missing ages using forward and backward filling within each user group.
- **Details**:
  - Sorts the dataframe by 'User_ID' and 'age'.
  - Fills missing ages within each user group using forward and backward filling.
  - Updates the 'age_group' column based on the filled ages.

## 8. Merging Age Data

**Function**: `merge_age_data`

- **Purpose**: Merges age data from the small dataframe into the main dataframe.
- **Details**:
  - Drops existing 'age' and 'age_group' columns from the main dataframe.
  - Maps 'age' and 'age_group' values from the small dataframe to the main dataframe based on 'User_ID'.

## 9. Predicting and Filling Ages

**Function**: `predict_and_fill_ages`

- **Purpose**: Predicts and fills missing ages using a linear regression model.
- **Details**:
  - Selects relevant columns for prediction.
  - Splits the data into training and testing sets.
  - Uses a pipeline with preprocessing and a linear regression model to predict ages.
  - Fills missing ages in the main dataframe based on predictions.
  - Updates the 'age_group' column based on the predicted ages.

## 10. Converting Columns to Numeric

**Function**: `convert_columns_to_numeric`

- **Purpose**: Converts specified columns to numeric type in the DataFrame.
- **Details**:
  - Attempts to convert specified columns to numeric type.
  - Handles conversion errors and coerces invalid values to NaN.

## 11. Merging Additional Data

**Function**: `merge_additional_data`

- **Purpose**: Merges additional data into 'df_large' based on specified merge columns.
- **Details**:
  - Identifies and handles data type incompatibilities between the dataframes.
  - Merges the main dataframe with additional data on specified columns.

## 12. Filtering Data

**Function**: `filter_data`

- **Purpose**: Filters the merged data to include only rows with relevant columns.
- **Details**:
  - Selects and retains columns related to order information.

## 13. Saving Data

**Function**: `save_data`

- **Purpose**: Saves the merged data to a CSV file.
- **Details**:
  - Writes the final merged and filtered dataframe to a CSV file.

## Main Function

**Function**: `main`

- **Purpose**: Orchestrates the entire data processing pipeline.
- **Details**:
  - Calls all the defined functions in the correct order to download, load, clean, process, merge, and save the data.

---
"""

import pandas as pd
import gdown
import re
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder

all_df_filtered = None
f=None
def download_files():
    file_ids = [
        ('125NiVk2lPj6GngW04Zp3gJiL5LluIVhr', 'after1.csv'),
        ('1GD_A7KQuOjmQFkzZ4ogM510zUj_CpBIB', 'before.csv')
    ]
    for file_id, destination in file_ids:
        gdown.download(f'https://drive.google.com/uc?id={file_id}', destination, quiet=False)

def load_data():
    data = pd.read_csv("/content/drive/MyDrive/TwentyToo/all_filtered_df1&df3.csv")
    df1 = pd.read_csv('after1.csv')
    df3 = pd.read_csv('before.csv')
    return df1, df3, data

def clean_data(df1):
    f = df1.drop_duplicates()
    f = f.dropna(subset=['Answer'])
    return f

def extract_age(answer):
    if not isinstance(answer, str):
        return None

    birthdate_match = re.findall(r'\b\d{2}-\d{2}-\d{4}\b', answer)
    if birthdate_match:
        birthdate = datetime.strptime(birthdate_match[0], '%d-%m-%Y')
        today = datetime.today()
        age = today.year - birthdate.year - ((today.month, today.day) < (birthdate.month, birthdate.day))
        return age if 0 <= age <= 120 else None

    return None

def add_age_and_age_group(df1_cleaned):
    df1_cleaned['age'] = df1_cleaned['Answer'].apply(extract_age)
    bins = [0, 12, 18, 35, 60, 100]
    labels = ['Child', 'Teen', 'Adult', 'Middle Age', 'Senior']
    df1_cleaned['age_group'] = pd.cut(df1_cleaned['age'], bins=bins, labels=labels, right=False)
    return df1_cleaned

def extract_unique_users(df1_cleaned):
    subset_columns = ['User_ID', 'age', 'age_group', 'city', 'region']
    extracted_data = df1_cleaned[subset_columns]
    df_cleaned = extracted_data.dropna(subset=['User_ID'])
    unique_users = df_cleaned.drop_duplicates(subset=['User_ID', 'age', 'city', 'region'])
    return unique_users

def fill_missing_ages(unique_users):
    unique_users.dropna(subset=['User_ID'], inplace=True)
    unique_users = unique_users.sort_values(by=['User_ID', 'age'], na_position='last')
    grouped = unique_users.groupby('User_ID')

    def fill_age_group(group):
        group['age'] = group['age'].fillna(method='ffill').fillna(method='bfill')
        bins = [0, 12, 18, 35, 60, 100]
        labels = ['Child', 'Teen', 'Adult', 'Middle Age', 'Senior']
        group['age_group'] = pd.cut(group['age'], bins=bins, labels=labels, right=False)
        return group

    filled_groups = grouped.apply(fill_age_group)
    filled_groups.reset_index(drop=True, inplace=True)
    return filled_groups

def merge_age_data(df3, df_small):
    columns_to_drop = [col for col in ['age', 'age_group'] if col in df3.columns]
    df3.drop(columns=columns_to_drop, inplace=True)
    age_dict = pd.Series(df_small.age.values, index=df_small.User_ID).to_dict()
    age_group_dict = pd.Series(df_small.age_group.values, index=df_small.User_ID).to_dict()
    df3['age'] = df3['User_ID'].map(age_dict)
    df3['age_group'] = df3['User_ID'].map(age_group_dict)
    #df3 = df3.drop_duplicates()
    return df3

def predict_and_fill_ages(df_large):
    columns_to_use = ['city', 'region', 'event_name', 'Answer', 'question', 'button_name1']
    df_filtered = df_large[columns_to_use + ['age']].dropna(subset=['age'])
    X = df_filtered.drop(columns=['age'])
    y = df_filtered['age']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    categorical_features = ['city', 'region', 'event_name', 'Answer', 'question', 'button_name1']
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    preprocessor = ColumnTransformer(
        transformers=[
            ('cat', categorical_transformer, categorical_features)
        ])

    lr_regressor = Pipeline(steps=[('preprocessor', preprocessor),
                                   ('regressor', LinearRegression())])

    lr_regressor.fit(X_train, y_train)

    X_missing_age = df_large[df_large['age'].isnull()][columns_to_use]
    predicted_ages = lr_regressor.predict(X_missing_age)
    df_large.loc[df_large['age'].isnull(), 'age'] = predicted_ages

    missing_values = df_large['age'].isnull().sum()
    print(f"Missing ages after imputation: {missing_values}")

    bins = [0, 12, 18, 35, 60, 100]
    labels = ['Child', 'Teen', 'Adult', 'Middle Age', 'Senior']
    df_large['age_group'] = pd.cut(df_large['age'], bins=bins, labels=labels, right=False)
    return df_large


def convert_columns_to_numeric(data):
    """
    Convert specified columns to numeric type in DataFrame.
    """
    columns_to_convert = ['Page_Name', 'Popup_Name', 'Choice', 'Answer', 'Rating_Result', 'sort_values',
                          'Filter_Values', 'product_ids', 'session_time', 'question', 'section_category',
                          'view_section_category', 'view_section_name', 'explore_section_name']
    for col in columns_to_convert:
        try:
            data[col] = pd.to_numeric(data[col], errors='coerce')
        except ValueError as e:
            print(f"Error converting {col}: {e}")
    return data

def merge_additional_data(df_large, data):
    """
    Merge additional data into 'df_large' based on specified merge columns.
    """
    columns_to_merge = ['product_id_cleaned']
    merge_columns = ['time', 'event_name', 'distinct_id', 'carrier', 'city', 'device_id',
                     'model', 'region', 'duration', 'Page_Name', 'User_ID',
                     'products_Number', 'ae_session_length', 'Popup_Name', 'Choice',
                     'Answer', 'product_package', 'Rating_Result', 'from_where',
                     'sort_values', 'Filter_Values', 'product_ids', 'session_time',
                     'question', 'value', 'section_category', 'section_name', 'button_name1',
                     'event_category', 'screen_name', 'event_month', 'event_week',
                     'event_day', 'event_year', 'event_date', 'view_section_category',
                     'view_section_name', 'explore_section_name', 'order_id', 'order_status',
                     'number_of_items', 'order_price', 'order_refund', 'unique_id']

    data_unique = data.drop_duplicates(subset=merge_columns)

    for col in merge_columns:
      if df_large[col].dtype != data_unique[col].dtype:
        # Attempt to convert to numeric if one is numeric and the other is not
        if df_large[col].dtype in ['int64', 'float64'] and data_unique[col].dtype == 'object':
            try:
                data_unique[col] = pd.to_numeric(data_unique[col], errors='coerce')
            except ValueError:
                print(f"Could not convert column {col} in data_unique to numeric.")
        elif df_large[col].dtype == 'object' and data_unique[col].dtype in ['int64', 'float64']:
            try:
                df_large[col] = pd.to_numeric(df_large[col], errors='coerce')
            except ValueError:
                print(f"Could not convert column {col} in data3 to numeric.")
        else:
            print(f"Column {col} has incompatible data types and cannot be automatically converted.")

    merged_data = pd.merge(df_large,
                           data_unique[merge_columns + columns_to_merge],
                           on=merge_columns,
                           how='left')
    return merged_data

def filter_data(merged_data):
    """
    Filter 'merged_data' based on specified events and button names of interest.
    """
    events_of_interest = [
        "product_similar_items_clicking",
        "product_details_page_2_view",
        "product_details_page_View",
        "generic_clicking",
        "ButtonClick",
        "shopping_bag_clicking"
    ]
    button_names_of_interest = [
        "product_itself",
        "heart_button",
        "like_button",
        "bag_button",
        "plus_button",
        "contains\"AddToFavorite\"",
        "N/A"  # Include missing values as a valid category
    ]
    filtered_df = merged_data[(merged_data['event_name'].isin(events_of_interest)) |
                              (merged_data['button_name1'].isin(button_names_of_interest))]
    return filtered_df

def apply_interaction_mapping(data):
  # Mapping of event_name and button_name1 to interaction categories
    interaction_mapping = {
      ('product_similar_items_clicking', 'product_itself'): ('Products clicks', 1),
      ('product_details_page_2_view', 'N/A'): ('Products clicks', 1),
      ('product_details_page_View', 'N/A'): ('Products clicks', 1),
      ('generic_clicking', 'heart_button'): ('Add to wishlist', 2),
      ('ButtonClick', 'contains"AddToFavorite"'): ('Add to wishlist', 2),
      ('product_similar_items_clicking', 'heart_button'): ('Add to wishlist', 2),
      ('shopping_bag_clicking', 'like_button'): ('Add to wishlist', 2),
      ('product_similar_items_clicking', 'bag_button'): ('Add to cart', 3),
      ('generic_clicking', 'bag_button'): ('Add to cart', 3),
      ('shopping_bag_clicking', 'plus_button'): ('Add to cart', 3)
    }
    # Function to get interaction weight based on event_name and button_name1
    def get_interaction(event_name, button_name1):
        return interaction_mapping.get((event_name, button_name1), (None, 0))[1]

    # Create the interaction column
    data['interaction'] = data.apply(lambda row: get_interaction(row['event_name'], row['button_name1']), axis=1)

    # Drop rows where interaction is 0 (not in the mapping)
    data = data[data['interaction'] > 0]

    return data

def main():
    global all_df_filtered,f
    download_files()
    df1, df3 ,data = load_data()
    filtered_dfs = []

    for idx, df in enumerate([df1, df3], start=1):
      df1_cleaned = clean_data(df)
      df1_with_age = add_age_and_age_group(df1_cleaned)
      unique_users = extract_unique_users(df1_with_age)
      filled_groups = fill_missing_ages(unique_users)
      df3_merged = merge_age_data(df, filled_groups)
      df_large = predict_and_fill_ages(df3_merged)
      #print(df_large)
      #print(df_large.describe())
      #df_large.to_csv('df_large.csv', index=False)
      df_large = convert_columns_to_numeric(df_large)

      merged_data = merge_additional_data(df_large, data)
      filtered_data = filter_data(merged_data)
      #print(filtered_data)
      #filtered_data.to_csv('filtered_data.csv', index=False)
      filtered_dfs.append(filtered_data)
    all_df_filtered = pd.concat(filtered_dfs, ignore_index=True)
    #print(all_df_filtered)
    f=all_df_filtered.drop_duplicates()
    columns_to_select = ['city', 'region', 'User_ID', 'button_name1', 'age', 'age_group', 'product_id_cleaned', 'event_name']
    f = f[columns_to_select]
    f=f.dropna(subset=['User_ID'])
    f=apply_interaction_mapping(f)

    f['region'].fillna(f['city'], inplace=True)

    # Fill nulls in 'city' with 'region' values
    f['city'].fillna(f['region'], inplace=True)

    # Sort DataFrame by 'User_ID' to ensure consecutive rows of the same user are together
    f.sort_values(by='User_ID', inplace=True)

    # Forward fill the missing values for 'city' within each user group
    f['city'] = f.groupby('User_ID')['city'].ffill()

    # Backward fill any remaining missing values (in case the first entry for a user is NaN)
    f['city'] = f.groupby('User_ID')['city'].bfill()
    # Forward fill the missing values for 'region' within each user group
    f['region'] = f.groupby('User_ID')['region'].ffill()

    # Backward fill any remaining missing values (in case the first entry for a user is NaN)
    f['region'] = f.groupby('User_ID')['region'].bfill()
    print(f)
if __name__ == "__main__":
    main()



