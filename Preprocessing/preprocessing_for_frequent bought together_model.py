# -*- coding: utf-8 -*-
"""preprocessing for first model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1etlDnvilOxFrcq5hVo2DGg3Lu22QJEM0

# Data Processing Script

## Overview

This script downloads data files from Google Drive, processes the data to clean and extract product IDs, filters the data based on specific events and button names, and saves the processed data to CSV files. The processing is done in chunks to handle large datasets efficiently.

## Dependencies

- `gdown`: For downloading files from Google Drive.
- `pandas`: For data manipulation and processing.
- `re`: For regular expression operations.

Install the required packages using:

```sh
pip install gdown pandas
```

## Functions

### `download_files(file_ids)`

Downloads files from Google Drive using their file IDs.

- **Arguments**:
  - `file_ids` (list of tuples): List of tuples containing file IDs and destination file names.
  
- **Example**:
  ```python
  file_ids = [
      ('file_id_1', 'destination_1.csv'),
      ('file_id_2', 'destination_2.csv')
  ]
  download_files(file_ids)
  ```

### `extract_all_ids(product_ids)`

Extracts all alphanumeric IDs from the given string.

- **Arguments**:
  - `product_ids` (str): String containing product IDs.
  
- **Returns**:
  - `list of str`: List of extracted alphanumeric IDs, or `None` if no matches are found.

### `process_chunk(chunk)`

Cleans the product IDs and explodes the lists into separate rows for a chunk of data.

- **Arguments**:
  - `chunk` (pd.DataFrame): DataFrame chunk to process.
  
- **Returns**:
  - `pd.DataFrame`: DataFrame with exploded product IDs.

### `filter_dataframe(df, events_of_interest, button_names_of_interest)`

Filters the DataFrame based on specified events and button names.

- **Arguments**:
  - `df` (pd.DataFrame): DataFrame to filter.
  - `events_of_interest` (list of str): List of event names to filter.
  - `button_names_of_interest` (list of str): List of button names to filter.
  
- **Returns**:
  - `pd.DataFrame`: Filtered DataFrame.

### `process_files_in_chunks(filenames, chunksize=100000)`

Processes files in chunks to handle large datasets efficiently.

- **Arguments**:
  - `filenames` (list of str): List of file names to process.
  - `chunksize` (int, optional): Size of chunks to process. Default is 100,000.
  
- **Returns**:
  - `pd.DataFrame`: Concatenated DataFrame of processed chunks.

## Main Script

The main script performs the following steps:

1. **Download Files**:
   - Downloads the specified files from Google Drive.
   
2. **Process Files in Chunks**:
   - Processes the downloaded files in chunks to clean and explode product IDs.
   
3. **Filter DataFrames**:
   - Filters the processed DataFrames based on specified events and button names.
   
4. **Concatenate Filtered DataFrames**:
   - Concatenates the filtered DataFrames into a single DataFrame.
   
5. **Extract Unique Product IDs**:
   - Extracts and saves unique product IDs from the concatenated DataFrame.
   
6. **Save Processed Data**:
   - Saves the filtered and concatenated DataFrame to a CSV file.
"""

import gdown
import pandas as pd
import re

def download_files(file_ids):
    for file_id, destination in file_ids:
        gdown.download(f'https://drive.google.com/uc?id={file_id}', destination, quiet=False)

def extract_all_ids(product_ids):
    if pd.isna(product_ids):
        return None
    pattern = r'[a-fA-F0-9]{32,}'
    matches = re.findall(pattern, product_ids)
    return matches if matches else None

def process_chunk(chunk):
    chunk['product_id_cleaned'] = chunk['product_package'].apply(extract_all_ids)
    chunk['product_id_cleaned2'] = chunk['product_ids'].apply(extract_all_ids)
    return chunk.explode('product_id_cleaned').explode('product_id_cleaned2')

def filter_dataframe(df, events_of_interest, button_names_of_interest):
    return df[(df['event_name'].isin(events_of_interest)) | (df['button_name1'].isin(button_names_of_interest))]

def process_files_in_chunks(filenames, chunksize=100000):
    processed_chunks = []
    for filename in filenames:
        chunk_iter = pd.read_csv(filename, chunksize=chunksize)
        for chunk in chunk_iter:
            processed_chunk = process_chunk(chunk)
            processed_chunks.append(processed_chunk)
    return pd.concat(processed_chunks)

def main():
    file_ids = [
        ('125NiVk2lPj6GngW04Zp3gJiL5LluIVhr', 'after1.csv'),
        ('1GD_A7KQuOjmQFkzZ4ogM510zUj_CpBIB', 'before.csv')
    ]

    download_files(file_ids)

    filenames = ['after1.csv', 'before.csv']
    processed_df1 = process_files_in_chunks([filenames[0]])
    processed_df2 = process_files_in_chunks([filenames[1]])

    events_of_interest = [
        "product_similar_items_clicking",
        "product_details_page_2_view",
        "product_details_page_View",
        "generic_clicking",
        "ButtonClick",
        "shopping_bag_clicking"
    ]

    button_names_of_interest = [
        "product_itself",
        "heart_button",
        "like_button",
        "bag_button",
        "plus_button",
        "contains\"AddToFavorite\"",
        "N/A"
    ]

    filtered_df1 = filter_dataframe(processed_df1, events_of_interest, button_names_of_interest)
    filtered_df2 = filter_dataframe(processed_df2, events_of_interest, button_names_of_interest)

    # Concatenate the filtered DataFrames
    all_df_filtered = pd.concat([filtered_df1, filtered_df2], ignore_index=True)

    unique_product_ids = pd.concat([all_df_filtered['product_id_cleaned'], all_df_filtered['product_id_cleaned2']])
    unique_product_ids = unique_product_ids.dropna().drop_duplicates().reset_index(drop=True)

    #print(unique_product_ids)

    unique_product_ids.to_csv('cleaned_product_ids.csv', index=False)
    all_df_filtered.to_csv('all_df_filtered.csv', index=False)
    # print(filtered_df1)
    # print(filtered_df2)
    # print(all_df_filtered)
    # print(all_df_filtered.describe())
if __name__ == '__main__':
    main()

